---
title: Generating JSONs with LLMs - Beyond Basics
description: Advanced techniques for improving speed, accuracy and cost of JSON generation with LLMs.
publishDate: "23 August 2024"
tags: ["llm", "ai", "json"]
---

import LLMParallelProcessing from "@/components/LLMParallelProcessing";

Turning unstructured data into structured data is one of the core use cases for AI. In the OpenAI APIs world, the three most popular methods are:

- **JSON mode** - Instructing an LLM to output JSON. Instructions regarding the shape of the JSON must included in the prompt. This is also available in [Mistral models](https://docs.mistral.ai/capabilities/json_mode/).
- **Tool calling** - Instructing an LLM to call a tool that follows a certain JSONSchema. This functionality is also available in Anthropic's Claude models family.
- **Structured Outputs** - Evolution of JSON Mode with strict schema adherance. This mode while guaranteeing that the output will be a valid JSON adhering to the schema, comes with a cold-start cost of creating a grammar increasing latency for the first request.

You can learn more about their strengths and weaknesses in my [previous blogpost about benchmarking Strict Mode](/posts/benchmarking-llms-for-structured-json-generation/).

## Increasing Accuracy

### Enriching the Schema

Coming up with more detailed, less vague schema is your first line of defense in increasing the accuracy of the output. This includes:

- Providing `descriptions` for each field. Think of them as mini prompts for each field.
- Providing `reasoning` for fields that require deducing, decision making or computing a value based on other fields. Think of it as mini ["Chain of Thought"](https://www.promptingguide.ai/techniques/cot) approach for each field. [Studies have shown](https://arxiv.org/abs/2201.11903) that providing reasoning can improve the accuracy of the output. (todo: something something about attention mechanism)
- Not all JSONSchema are supported by the strict mode! Solution to this is: create as detailed schema as possible. Pass dumbed down version of the schema with only supported properties to the LLM. Validate the output against the full schema using e.g. Zod. Get the validation errors and pass them to the LLM along with the output and ask it to fix the errors. (todo: provide an example)

**Cost:** Increased input tokens usage

### Fields Sequencing

In traditional computing, the order of fields in JSON does not matter but in LLM it does. Try to put them in as logical order as possible. For instance, when extracting data from invoice, bad order can be something like `"signed_by, invoice number, total, due date"` etc. If you were a human reading this invoice in that order, you would be very confused. LLM will have exactly the same trouble.

**Cost:** Free

### Multipass mode & election

This one is especiall useful in ambiguous cases and multimodal processing. Consider the following example from [`vidore/docvqa_test_subsampled` dataset](https://huggingface.co/datasets/vidore/docvqa_test_subsampled/viewer):

![Example from `vidore/docvqa_test_subsampled` dataset](./unclear-example.png "Example from `vidore/docvqa_test_subsampled` dataset")

- What's the passenger's surname? Is it "Darby" or "Darry"?
- Is the conjunction ticket number `0015353-530004` or `0018353-530004`?
- Form of payment is `AGT-CK` or `HGT-GK`?

LLM might not be able to answer all of these questions with certainty. In such cases, you can use the `n` parameter, which allows for generating multiple completions in one request. This can be used to generate multiple JSONs in a single request and pay for the input just once (you will still be billed a multiple of `n` for the output tokens). This combined with bigger `temperature` which increases models creativity, can be used to ensemble a "mixture of results" approach.

Once you have multiple JSONs, for each field you can elect to take the most frequent value by using majority voting to decide on the value.

![Election of the most frequent value](./election.png "Election of the most frequent value")

<details>
<summary>Example TS code for merging multiple JSONs</summary>

```typescript
import { ChatCompletion, ChatCompletionMessage } from 'openai';

type JsonValue = string | number | boolean | null | JsonArray | JsonObject;
type JsonArray = JsonValue[];
type JsonObject = { [key: string]: JsonValue };

function getMostFrequentValue(values: JsonValue[]): JsonValue {
  const counts = new Map<string, { count: number; value: JsonValue }>();
  let maxCount = 0;
  let mostFrequent: JsonValue;

  for (const value of values) {
    const stringValue = JSON.stringify(value);
    const current = counts.get(stringValue) || { count: 0, value };
    current.count++;
    counts.set(stringValue, current);

    if (current.count > maxCount) {
      maxCount = current.count;
      mostFrequent = current.value;
    }
  }

  return mostFrequent!;
}

function mergeJsonValues(values: JsonValue[]): JsonValue {
  if (values.length === 0) return null;
  
  if (values.every(v => Array.isArray(v))) {
    return mergeJsonArrays(values as JsonArray[]);
  }
  
  if (values.every(v => typeof v === 'object' && v !== null && !Array.isArray(v))) {
    return mergeJsonObjects(values as JsonObject[]);
  }
  
  return getMostFrequentValue(values);
}

function mergeJsonArrays(arrays: JsonArray[]): JsonArray {
  const maxLength = Math.max(...arrays.map(arr => arr.length));
  const result: JsonArray = [];

  for (let i = 0; i < maxLength; i++) {
    const values = arrays.map(arr => arr[i]).filter(v => v !== undefined);
    result.push(mergeJsonValues(values));
  }

  return result;
}

function mergeJsonObjects(objects: JsonObject[]): JsonObject {
  const result: JsonObject = {};

  for (const key of new Set(objects.flatMap(obj => Object.keys(obj)))) {
    const values = objects.map(obj => obj[key]).filter(v => v !== undefined);
    result[key] = mergeJsonValues(values);
  }

  return result;
}

function mergeChatCompletionChoices(completion: ChatCompletion): JsonValue {
  const parsedContents = completion.choices.map(choice => {
    const message = choice.message as ChatCompletionMessage;
    return JSON.parse(message.content || 'null');
  });

  return mergeJsonValues(parsedContents);
}

// Usage example
const mergedResult = mergeChatCompletionChoices(chatCompletionResponse);
console.log(mergedResult);
```
</details>

In models from other providers where you cannot use `n` parameter, you can still use the same approach by generating multiple completions. Keep in mind that the cost of such operation would be a multipled for both input and output tokens.

**Cost:** In OpenAI - multiplied output tokens usage. For other providers, multiplied output _and_ input tokens usage.

### Routing

> todo: specialized submodels

![Example of routing](./routing.png "Example of routing")



## Increasing Speed

A trick commonly used in traditional computing is to use parallel processing to speed up the operation. This can be also used in LLM world. Split your schema into multiple independent schemas and generate multiple JSONs in parallel. Merge them afterwards.

Example code in Typescript using `ai` library:

```typescript
import { generateObject } from "ai";
import { openai } from "@ai-sdk/openai";
import { z } from "zod";

// Define the subschemas
const subSchemaOne = z.object({
	name: z.string(),
});
const subSchemaTwo = z.object({
	age: z.number(),
});

// Generate multiple JSONs in parallel
const responses = await Promise.all(
	[subSchemaOne, subSchemaTwo].map((subschema) => {
		const { object } = generateObject(subschema, {
			model: openai("gpt-4o"),
			schema: subschema,
			prompt: "John is 25 years old. What's his name or age?",
		});

		return object;
	}),
);

// Merge the responses into a single JSON object
const merged = responses.reduce((acc, curr) => ({ ...acc, ...curr }), {});
```

#### Caveats 

Be careful though. As I mentioned before, sequence of fields and their co-presence does matter in LLM world. Make sure that your groups are independent logically and that they don't have any dependencies on each other. In practical example - if you have an invoice, you can split it into following independent subschemas: `Recipient`, `Sender`, `Items`.

Moreover, like classical computing, this is also a subject to [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl%27s_law) or _the law of diminishing returns_ with slight twist:

<LLMParallelProcessing client:visible />



**Cost:** Multiplied input tokens usage.

## Reducing Costs

If you don't want to sacrifice accuracy, the only option you can go here is to use a cheaper & smaller models.


